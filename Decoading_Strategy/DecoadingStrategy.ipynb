{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f908474e",
   "metadata": {},
   "source": [
    "# Decoading Streategy\n",
    "\n",
    "All a LLM do while generative text is predict the next word. To Predict the next word there are 3 major techinque:\n",
    "1. Gready Search\n",
    "2. Beam Search\n",
    "\n",
    "## Gready Search\n",
    "\n",
    " In this approch we select the word with highest probability. Let's try to implement the approch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a7ce15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Karmakar\\Desktop\\Course\\AI\\DecoadingStrategy\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "model_name = 'gpt2-medium'\n",
    "\n",
    "# loading tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152b8a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to finf the next word using gready search\n",
    "def get_next_word_using_gready_search(input_text, input_ids, time_steps, choices_per_step):\n",
    "    \"\"\"\n",
    "    Predict next word using Gready Search.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): Input text Sequence.\n",
    "        input_ids (torch.Tensor): Input Tensor containing tensorized text sequence\n",
    "        time_steps (int): Number of next steps to predict\n",
    "        choices_per_step (int): Number of choice at each step.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    iterations = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(time_steps):\n",
    "            iteration = dict()\n",
    "            iteration['input'] = tokenizer.decode(input_ids[0])\n",
    "            \n",
    "            # Predicting using model\n",
    "            output = model(input_ids = input_ids)\n",
    "            next_token_logits = output.logits[0,-1,:]\n",
    "            next_token_probability = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "            sorted_index_of_next_probability = torch.argsort(next_token_probability, descending=True, dim=-1)\n",
    "\n",
    "            # Top few highest tokens\n",
    "            for choics_idx in range(choices_per_step):\n",
    "                token_index_sorted = sorted_index_of_next_probability[choics_idx]\n",
    "                token_prob = next_token_probability[token_index_sorted].cpu().numpy()\n",
    "                token_choice = f\"{tokenizer.decode(token_index_sorted)} ({token_prob*100:.2f}%)\"\n",
    "                iteration[f'Choice {choics_idx+1}'] = token_choice\n",
    "            \n",
    "            # Appending predicted next token to input\n",
    "            input_ids = torch.cat([input_ids, sorted_index_of_next_probability[None, 0, None]], dim=-1)\n",
    "            # iterations.append(iteration)\n",
    "\n",
    "        # Returning iterations as df\n",
    "        # return pd.DataFrame(iterations)\n",
    "        return tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a710cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is a good day for the United States,\" he said. \"We're going to be able to continue to work with our allies and partners to make sure that we're able to continue to make progress on the\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Today is a\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "generated_text = get_next_word_using_gready_search(input_text=input_text,\n",
    "                                  input_ids=input_ids,\n",
    "                                  time_steps=40,\n",
    "                                  choices_per_step=5)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66087391",
   "metadata": {},
   "source": [
    "Lets generate Gready search result from transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "848c657d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is a beautiful day. I'm going to go to the beach and enjoy the sun. I'm going to go to the beach and enjoy the sun. I'm going to go to the beach and enjoy the sun. I\n"
     ]
    }
   ],
   "source": [
    "# Input Tokens\n",
    "input_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n",
    "\n",
    "# Generating data from model using Hugging face library\n",
    "output = model.generate(**input_tokens, max_new_tokens=40)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens= True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee908c57",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "\n",
    "This is also a deterministic search. Here instead of choosing the word with heighest probability we choose n words with heighest conditional probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b22e6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love it.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Beam search function\n",
    "def beam_search(prompt, max_length=50, num_beams=5):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input and move to device\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Initialize beams: Each beam starts with the same initial input\n",
    "    beams = [(input_ids, 0)]  # Tuple of (tokens, score)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        new_beams = []\n",
    "        # new_beams = beams\n",
    "        \n",
    "        for tokens, score in beams:\n",
    "            # Get model predictions\n",
    "            with torch.no_grad():\n",
    "                outputs = model(tokens)\n",
    "            \n",
    "            # Extract logits for the last token and apply softmax\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Get top beam candidates\n",
    "            top_probs, top_indices = probs.topk(num_beams)\n",
    "            \n",
    "            # Create new beams\n",
    "            for i in range(num_beams):\n",
    "                new_token = top_indices[:, i].unsqueeze(-1)\n",
    "                new_score = score + torch.log(top_probs[:, i])  # Update score\n",
    "                \n",
    "                new_beam = (torch.cat([tokens, new_token], dim=-1), new_score)\n",
    "                new_beams.append(new_beam)\n",
    "        \n",
    "        # Sort beams by score and keep the best ones\n",
    "        new_beams.sort(key=lambda x: x[1], reverse=True)\n",
    "        beams = new_beams[:num_beams]\n",
    "    \n",
    "    # Select the best final beam\n",
    "    best_tokens = beams[0][0]\n",
    "    return tokenizer.decode(best_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "input_text = \"i love\"\n",
    "generated_text = beam_search(input_text, max_length=2, num_beams=3)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fadc149",
   "metadata": {},
   "source": [
    "Trying Hugging face library to predict next word using beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0218545c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love it.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"i love\"\n",
    "\n",
    "# Input Tokens\n",
    "input_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n",
    "\n",
    "# Generating data from model using Hugging face library\n",
    "output = model.generate(**input_tokens, max_new_tokens=2, num_beams = 3)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6485891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
