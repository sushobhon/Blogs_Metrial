{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f908474e",
   "metadata": {},
   "source": [
    "# Decoading Streategy\n",
    "\n",
    "All a LLM do while generative text is predict the next word. To Predict the next word there are 3 major techinque:\n",
    "1. Gready Search\n",
    "2. Beam Search\n",
    "\n",
    "## Gready Search\n",
    "\n",
    " In this approch we select the word with highest probability. Let's try to implement the approch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91267d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hf_xet\n",
      "  Downloading hf_xet-1.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
      "Downloading hf_xet-1.0.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.0.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "# !pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a7ce15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "model_name = 'gpt2-medium'\n",
    "\n",
    "# loading tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "152b8a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to finf the next word using gready search\n",
    "def get_next_word_using_gready_search(input_text, input_ids, time_steps, choices_per_step):\n",
    "    \"\"\"\n",
    "    Predict next word using Gready Search.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): Input text Sequence.\n",
    "        input_ids (torch.Tensor): Input Tensor containing tensorized text sequence\n",
    "        time_steps (int): Number of next steps to predict\n",
    "        choices_per_step (int): Number of choice at each step.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    iterations = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(time_steps):\n",
    "            iteration = dict()\n",
    "            iteration['input'] = tokenizer.decode(input_ids[0])\n",
    "            \n",
    "            # Predicting using model\n",
    "            output = model(input_ids = input_ids)\n",
    "            next_token_logits = output.logits[0,-1,:]\n",
    "            next_token_probability = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "            sorted_index_of_next_probability = torch.argsort(next_token_probability, descending=True, dim=-1)\n",
    "\n",
    "            # Top few highest tokens\n",
    "            for choics_idx in range(choices_per_step):\n",
    "                token_index_sorted = sorted_index_of_next_probability[choics_idx]\n",
    "                token_prob = next_token_probability[token_index_sorted].cpu().numpy()\n",
    "                token_choice = f\"{tokenizer.decode(token_index_sorted)} ({token_prob*100:.2f}%)\"\n",
    "                iteration[f'Choice {choics_idx+1}'] = token_choice\n",
    "            \n",
    "            # Appending predicted next token to input\n",
    "            input_ids = torch.cat([input_ids, sorted_index_of_next_probability[None, 0, None]], dim=-1)\n",
    "            # iterations.append(iteration)\n",
    "\n",
    "        # Returning iterations as df\n",
    "        # return pd.DataFrame(iterations)\n",
    "        return tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a710cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love the idea\n"
     ]
    }
   ],
   "source": [
    "input_text = \"i love\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "generated_text = get_next_word_using_gready_search(input_text=input_text,\n",
    "                                  input_ids=input_ids,\n",
    "                                  time_steps=2,\n",
    "                                  choices_per_step=1)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66087391",
   "metadata": {},
   "source": [
    "Lets generate Gready search result from transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "848c657d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love the idea\n"
     ]
    }
   ],
   "source": [
    "# Input Tokens\n",
    "input_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n",
    "\n",
    "# Generating data from model using Hugging face library\n",
    "output = model.generate(**input_tokens, max_new_tokens=2)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens= True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee908c57",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "\n",
    "This is also a deterministic search. Here instead of choosing the word with heighest probability we choose n words with heighest conditional probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b22e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Beam search function\n",
    "def beam_search(prompt, max_length=50, num_beams=5):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input and move to device\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Initialize beams: Each beam starts with the same initial input\n",
    "    beams = [(input_ids, 0)]  # Tuple of (tokens, score)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        new_beams = []\n",
    "        # new_beams = beams\n",
    "        \n",
    "        for tokens, score in beams:\n",
    "            # Get model predictions\n",
    "            with torch.no_grad():\n",
    "                outputs = model(tokens)\n",
    "            \n",
    "            # Extract logits for the last token and apply softmax\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Get top beam candidates\n",
    "            top_probs, top_indices = probs.topk(num_beams)\n",
    "            \n",
    "            # Create new beams\n",
    "            for i in range(num_beams):\n",
    "                new_token = top_indices[:, i].unsqueeze(-1)\n",
    "                new_score = score + torch.log(top_probs[:, i])  # Update score\n",
    "                \n",
    "                new_beam = (torch.cat([tokens, new_token], dim=-1), new_score)\n",
    "                new_beams.append(new_beam)\n",
    "        \n",
    "        # Sort beams by score and keep the best ones\n",
    "        new_beams.sort(key=lambda x: x[1], reverse=True)\n",
    "        beams = new_beams[:num_beams]\n",
    "    \n",
    "    # Select the best final beam\n",
    "    best_tokens = beams[0][0]\n",
    "    return tokenizer.decode(best_tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4827bd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love the idea\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_text = \"i love\"\n",
    "generated_text = beam_search(input_text, max_length=2, num_beams=1)  # num_beams = 1 is same as gready search\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2925813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love it.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_text = \"i love\"\n",
    "generated_text = beam_search(input_text, max_length=2, num_beams=3)  # num_beams = 1 is same as gready search\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fadc149",
   "metadata": {},
   "source": [
    "Trying Hugging face library to predict next word using beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0218545c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love it.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"i love\"\n",
    "\n",
    "# Input Tokens\n",
    "input_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n",
    "\n",
    "# Generating data from model using Hugging face library\n",
    "output = model.generate(**input_tokens, max_new_tokens=2, num_beams = 3)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens= True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d500b0b1",
   "metadata": {},
   "source": [
    "## TOP-K Samplling\n",
    "\n",
    "This is not a deterministic approch. In this approch:\n",
    "- 1. we predict the probability of all the token.\n",
    "- 2. Then we filter out top k token with heighest probability.\n",
    "- 3. Then we normalize the probability of top K tokens.\n",
    "- 4. Then Select a token Randomly based on the normalized token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f6485891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Final generated Text is:\n",
      "i love the way he's doing this, but I'm\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Beam search function\n",
    "def top_k_search(prompt, max_length=50, k=5, show_option= False):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input and move to device\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # output_tokens\n",
    "    output_tokens = input_ids\n",
    "\n",
    "    for _ in range(max_length):\n",
    "      if show_option:\n",
    "        print(f\"\\nFor {_+1} Token:\")\n",
    "        print(\"-\"*100)\n",
    "\n",
    "      # Get model predictions\n",
    "      with torch.no_grad():\n",
    "          outputs = model(output_tokens)\n",
    "      \n",
    "      # Extract logits for the last token and apply softmax\n",
    "      logits = outputs.logits[:, -1, :]\n",
    "      probs = torch.softmax(logits, dim=-1)\n",
    "      \n",
    "      # Get top beam candidates\n",
    "      top_probs, top_indices = probs.topk(k)\n",
    "\n",
    "      # Normalizing top K probabilities\n",
    "      top_probs_norm = top_probs/torch.sum(top_probs)\n",
    "      # Choosing an element randomly based on normalized probability of top k tokens\n",
    "      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n",
    "      selected_token = top_indices[0][selected_token_id.item()]\n",
    "      \n",
    "      # Reshape selected_token to have shape (1, 1)\n",
    "      selected_token = selected_token.unsqueeze(0).unsqueeze(0) # This will reshape selected token to (1,1) and will solve the error.\n",
    "\n",
    "      # Appending selected  token with input token\n",
    "      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n",
    "      \n",
    "      if show_option:\n",
    "        # Printing Top K tokens\n",
    "        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n",
    "        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n",
    "            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n",
    "      \n",
    "    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) # decode function works on 1D array only.\n",
    "\n",
    "# Example usage\n",
    "input_text = \"i love\"\n",
    "generated_text = top_k_search(input_text, max_length=10, k=5)  # num_beams = 1 is same as gready search\n",
    "print(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b05a328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "i love the way that the music\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "from transformers import set_seed\n",
    "# set_seed(42)\n",
    "\n",
    "input_text = \"i love the\"\n",
    "\n",
    "# Input Tokens\n",
    "input_tokens = tokenizer(input_text, return_tensors= 'pt').to(device)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    **input_tokens,\n",
    "    max_new_tokens=4,\n",
    "    do_sample=True,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532d322",
   "metadata": {},
   "source": [
    "## Top-p samplling\n",
    "\n",
    "This is not a deterministic approch. In this approch:\n",
    "- 1. we predict the probability of all the token.\n",
    "- 2. Then we filter out top p token with heighest probability.\n",
    "- 3. Then we normalize the probability of top K tokens.\n",
    "- 4. Then Select a token Randomly based on the normalized token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "57c84f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Final generated Text is:\n",
      "i love the new beer in this beer, and I'm\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Beam search function\n",
    "def top_p_search(prompt, max_length=50, p=1, show_option= False):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input and move to device\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # output_tokens\n",
    "    output_tokens = input_ids\n",
    "\n",
    "    for _ in range(max_length):\n",
    "      if show_option:\n",
    "        print(f\"\\nFor {_+1} Token:\")\n",
    "        print(\"-\"*100)\n",
    "\n",
    "      # Get model predictions\n",
    "      with torch.no_grad():\n",
    "          outputs = model(output_tokens)\n",
    "      \n",
    "      # Extract logits for the last token and apply softmax\n",
    "      logits = outputs.logits[:, -1, :]\n",
    "      probs = torch.softmax(logits, dim=-1)\n",
    "      \n",
    "      # Get top beam candidates\n",
    "      probs, indices = torch.sort(probs, dim = 1, descending=True)\n",
    "      cumulative_prob = torch.cumsum(probs[0], dim = 0)\n",
    "      top_probs = probs[:,:torch.sum(cumulative_prob<=p).item() + 1]\n",
    "      top_indices = indices[:,:torch.sum(cumulative_prob<=p).item() + 1]\n",
    "\n",
    "      # Normalizing top K probabilities\n",
    "      top_probs_norm = top_probs/torch.sum(top_probs)\n",
    "      # Choosing an element randomly based on normalized probability of top k tokens\n",
    "      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n",
    "      selected_token = top_indices[0][selected_token_id.item()]\n",
    "      \n",
    "      # Reshape selected_token to have shape (1, 1)\n",
    "      selected_token = selected_token.unsqueeze(0).unsqueeze(0) # This will reshape selected token to (1,1) and will solve the error.\n",
    "\n",
    "      # Appending selected  token with input token\n",
    "      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n",
    "      \n",
    "      if show_option:\n",
    "        # Printing Top K tokens\n",
    "        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n",
    "        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n",
    "            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n",
    "      \n",
    "    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) # decode function works on 1D array only.\n",
    "\n",
    "# Example usage\n",
    "input_text = \"i love\"\n",
    "generated_text = top_p_search(input_text, max_length=10, p=0.4)  \n",
    "print(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dc109b",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "While calculation probability from logits score consider temerature parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10ae5e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Defining updated softmax for PyTorch tensors\n",
    "def softmax_tensor(logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies the softmax function to a PyTorch tensor along the last dimension,\n",
    "    optionally with a temperature scaling factor.\n",
    "\n",
    "    Args:\n",
    "        logits: The input PyTorch tensor of logits.\n",
    "        temperature: A scaling factor for the logits (default: 1.0).\n",
    "\n",
    "    Returns:\n",
    "        A PyTorch tensor of the same shape as the input, with probabilities\n",
    "        along the last dimension.\n",
    "    \"\"\"\n",
    "    return F.softmax(logits / temperature, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d8cc05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Final generated Text is:\n",
      "i love the new beer in this beer, and I'm\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Beam search function\n",
    "def top_p_search(prompt, max_length=50, p=1, temperature = 1, show_option = False):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input and move to device\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # output_tokens\n",
    "    output_tokens = input_ids\n",
    "\n",
    "    for _ in range(max_length):\n",
    "      if show_option:\n",
    "        print(f\"\\nFor {_+1} Token:\")\n",
    "        print(\"-\"*100)\n",
    "\n",
    "      # Get model predictions\n",
    "      with torch.no_grad():\n",
    "          outputs = model(output_tokens)\n",
    "      \n",
    "      # Extract logits for the last token and apply softmax\n",
    "      logits = outputs.logits[:, -1, :]\n",
    "      #   probs = torch.softmax(logits, dim=-1)\n",
    "      probs = softmax_tensor(logits= logits, temperature= temperature + 1e-6)  # Added temperature while calculating probability\n",
    "      \n",
    "      # Get top beam candidates\n",
    "      probs, indices = torch.sort(probs, dim = 1, descending=True)\n",
    "      cumulative_prob = torch.cumsum(probs[0], dim = 0)\n",
    "      top_probs = probs[:,:torch.sum(cumulative_prob<=p).item() + 1]\n",
    "      top_indices = indices[:,:torch.sum(cumulative_prob<=p).item() + 1]\n",
    "\n",
    "      # Normalizing top K probabilities\n",
    "      top_probs_norm = top_probs/torch.sum(top_probs)\n",
    "\n",
    "      # Choosing an element randomly based on normalized probability of top k tokens\n",
    "      selected_token_id = torch.multinomial(top_probs_norm[0], num_samples=1, replacement= True)\n",
    "      selected_token = top_indices[0][selected_token_id.item()]\n",
    "      \n",
    "      # Reshape selected_token to have shape (1, 1)\n",
    "      selected_token = selected_token.unsqueeze(0).unsqueeze(0) # This will reshape selected token to (1,1) and will solve the error.\n",
    "\n",
    "      # Appending selected  token with input token\n",
    "      output_tokens = torch.cat((output_tokens, selected_token), dim=1)\n",
    "      \n",
    "      if show_option:\n",
    "        # Printing Top K tokens\n",
    "        print(f\"Selected Token: {tokenizer.decode(selected_token.item())}\\nTop K tokens are:\\n\")\n",
    "        for index, probability in zip(top_indices.squeeze(0), top_probs_norm.squeeze(0)):\n",
    "            print(f\"{tokenizer.decode(index.item())} ({round(probability.item() * 100, 2)}%)\")\n",
    "      \n",
    "    return tokenizer.decode(output_tokens[0], skip_special_tokens=True) # decode function works on 1D array only.\n",
    "\n",
    "# Example usage\n",
    "input_text = \"i love\"\n",
    "generated_text = top_p_search(input_text, max_length=10, p=0.4, temperature=1)  \n",
    "print(\"-\"*20 + \"\\nFinal generated Text is:\\n\" + generated_text + \"\\n\" + \"-\"*20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
